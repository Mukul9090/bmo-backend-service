name: CI/CD Pipeline - Deploy to Kubernetes

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:

env:
  DOCKER_IMAGE: mukul1599/backend-service
  DOCKER_TAG: latest
  K8S_CLUSTER_HOT_CONTEXT: hot-cluster
  K8S_CLUSTER_STANDBY_CONTEXT: standby-cluster
  HAPROXY_CLUSTER_CONTEXT: hot-cluster
  REPLICAS: 2

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: [self-hosted, macOS, ARM64]
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-buildx-action@v3
      
      - name: Test Docker Hub connectivity
        run: |
          for i in {1..3}; do
            curl -s --max-time 10 https://registry-1.docker.io/v2/ > /dev/null && exit 0 || sleep 5
          done
          exit 1
        timeout-minutes: 2

      - name: Login and Build
        uses: docker/login-action@v3
        timeout-minutes: 10
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push
        uses: docker/build-push-action@v5
        timeout-minutes: 45
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    name: Deploy to Kubernetes
    runs-on: [self-hosted, macOS, ARM64]
    needs: build-and-push
    if: github.event_name != 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Detect cluster contexts
        id: detect-contexts
        run: |
          # Get available contexts
          AVAILABLE_CONTEXTS=$(kubectl config get-contexts -o name 2>/dev/null || echo "")
          CURRENT_CONTEXT=$(kubectl config current-context 2>/dev/null || echo "")
          
          echo "Available contexts: $AVAILABLE_CONTEXTS"
          echo "Current context: $CURRENT_CONTEXT"
          
          # Check if specified contexts exist, otherwise use current context
          if kubectl config get-contexts ${{ env.K8S_CLUSTER_HOT_CONTEXT }} &>/dev/null; then
            HOT_CTX="${{ env.K8S_CLUSTER_HOT_CONTEXT }}"
            echo "âœ… Using hot context: $HOT_CTX"
          else
            HOT_CTX="${CURRENT_CONTEXT:-minikube}"
            echo "âš ï¸  Hot context '${{ env.K8S_CLUSTER_HOT_CONTEXT }}' not found, using: $HOT_CTX"
          fi
          
          if kubectl config get-contexts ${{ env.K8S_CLUSTER_STANDBY_CONTEXT }} &>/dev/null; then
            STANDBY_CTX="${{ env.K8S_CLUSTER_STANDBY_CONTEXT }}"
            echo "âœ… Using standby context: $STANDBY_CTX"
          else
            STANDBY_CTX="${CURRENT_CONTEXT:-minikube}"
            echo "âš ï¸  Standby context '${{ env.K8S_CLUSTER_STANDBY_CONTEXT }}' not found, using: $STANDBY_CTX"
          fi
          
          if kubectl config get-contexts ${{ env.HAPROXY_CLUSTER_CONTEXT }} &>/dev/null; then
            HAPROXY_CTX="${{ env.HAPROXY_CLUSTER_CONTEXT }}"
            echo "âœ… Using HAProxy context: $HAPROXY_CTX"
          else
            HAPROXY_CTX="${CURRENT_CONTEXT:-minikube}"
            echo "âš ï¸  HAProxy context '${{ env.HAPROXY_CLUSTER_CONTEXT }}' not found, using: $HAPROXY_CTX"
          fi
          
          echo "hot_context=$HOT_CTX" >> $GITHUB_OUTPUT
          echo "standby_context=$STANDBY_CTX" >> $GITHUB_OUTPUT
          echo "haproxy_context=$HAPROXY_CTX" >> $GITHUB_OUTPUT

      - name: Clean existing deployments
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
          echo "ðŸ§¹ Cleaning existing deployments..."
          
          # Clean hot cluster
          kubectl delete deployment backend-service --context=$HOT_CTX -n default --ignore-not-found=true --timeout=10s || true
          kubectl delete service backend-service --context=$HOT_CTX -n default --ignore-not-found=true --timeout=10s || true
          kubectl delete configmap backend-config --context=$HOT_CTX -n default --ignore-not-found=true --timeout=10s || true
          
          # Clean standby cluster
          kubectl delete deployment backend-service --context=$STANDBY_CTX -n default --ignore-not-found=true --timeout=10s || true
          kubectl delete service backend-service --context=$STANDBY_CTX -n default --ignore-not-found=true --timeout=10s || true
          kubectl delete configmap backend-config --context=$STANDBY_CTX -n default --ignore-not-found=true --timeout=10s || true
          
          # Clean HAProxy
          kubectl delete deployment haproxy --context=$HAPROXY_CTX --ignore-not-found=true --timeout=10s || true
          kubectl delete service haproxy --context=$HAPROXY_CTX --ignore-not-found=true --timeout=10s || true
          kubectl delete configmap haproxy-config --context=$HAPROXY_CTX --ignore-not-found=true --timeout=10s || true
          
          echo "âœ… Cleanup complete"

      - name: Deploy to clusters
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
          # Deploy hot cluster
          kubectl create configmap backend-config --from-literal=CLUSTER_ROLE=hot --context=$HOT_CTX -n default --dry-run=client -o yaml | kubectl apply --context=$HOT_CTX -f -
          sed "s|image: ${{ env.DOCKER_IMAGE }}$|image: ${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}|" k8s/cluster-hot/deployment.yaml | kubectl apply --context=$HOT_CTX -f -
          kubectl apply --context=$HOT_CTX -f k8s/cluster-hot/service.yaml
          
          # Wait for hot cluster pods to be ready
          echo "Waiting for hot cluster to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment/backend-service-hot --context=$HOT_CTX -n default || kubectl wait --for=condition=available --timeout=120s deployment/backend-service --context=$HOT_CTX -n default || true
          sleep 3
          
          # Deploy standby cluster
          kubectl create configmap backend-config --from-literal=CLUSTER_ROLE=standby --context=$STANDBY_CTX -n default --dry-run=client -o yaml | kubectl apply --context=$STANDBY_CTX -f -
          sed "s|image: ${{ env.DOCKER_IMAGE }}$|image: ${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}|" k8s/cluster-standby/deployment.yaml | kubectl apply --context=$STANDBY_CTX -f -
          kubectl apply --context=$STANDBY_CTX -f k8s/cluster-standby/service.yaml
          
          # If same context, we need to handle ConfigMap conflict
          # The second ConfigMap overwrites the first, so we need to set it back and restart pods
          if [ "$HOT_CTX" = "$STANDBY_CTX" ]; then
            echo "Same context detected - fixing ConfigMap conflict..."
            # Set hot ConfigMap again (standby overwrote it)
            kubectl create configmap backend-config --from-literal=CLUSTER_ROLE=hot --context=$HOT_CTX -n default --dry-run=client -o yaml | kubectl apply --context=$HOT_CTX -f -
            # Restart hot pods to pick up correct ConfigMap and wait for rollout to complete
            kubectl rollout restart deployment/backend-service-hot --context=$HOT_CTX -n default || kubectl rollout restart deployment/backend-service --context=$HOT_CTX -n default || true
            kubectl rollout status deployment/backend-service-hot --context=$HOT_CTX -n default --timeout=120s || kubectl rollout status deployment/backend-service --context=$HOT_CTX -n default --timeout=120s || true
            echo "âœ… Hot cluster pods restarted with correct ConfigMap"
            
            # Now set standby ConfigMap and restart standby pods
            kubectl create configmap backend-config --from-literal=CLUSTER_ROLE=standby --context=$STANDBY_CTX -n default --dry-run=client -o yaml | kubectl apply --context=$STANDBY_CTX -f -
            kubectl rollout restart deployment/backend-service --context=$STANDBY_CTX -n default
            kubectl rollout status deployment/backend-service --context=$STANDBY_CTX -n default --timeout=120s || true
            echo "âœ… Standby cluster pods restarted with correct ConfigMap"
          fi
          
          # Wait for services to exist (services don't have 'ready' condition)
          echo "Waiting for services to be created..."
          sleep 3
          kubectl get svc backend-service --context=$HOT_CTX -n default || echo "Hot service not found"
          kubectl get svc backend-service --context=$STANDBY_CTX -n default || echo "Standby service not found"
          
          # Get service endpoints - use ClusterIP (HAProxy can't resolve Kubernetes DNS)
          # Use separate service names to avoid conflicts when same cluster
          HOT_CLUSTERIP=$(kubectl get svc backend-service-hot --context=$HOT_CTX -n default -o jsonpath='{.spec.clusterIP}' 2>/dev/null || kubectl get svc backend-service --context=$HOT_CTX -n default -o jsonpath='{.spec.clusterIP}' 2>/dev/null || echo "")
          STANDBY_CLUSTERIP=$(kubectl get svc backend-service --context=$STANDBY_CTX -n default -o jsonpath='{.spec.clusterIP}' 2>/dev/null || echo "")
          
          if [ -n "$HOT_CLUSTERIP" ] && [ -n "$STANDBY_CLUSTERIP" ]; then
            # Use ClusterIPs (HAProxy can resolve IPs, not DNS names)
            HOT_ENDPOINT="${HOT_CLUSTERIP}:80"
            STANDBY_ENDPOINT="${STANDBY_CLUSTERIP}:80"
            if [ "$HOT_CTX" = "$STANDBY_CTX" ]; then
              echo "Same cluster - using ClusterIP: $HOT_ENDPOINT (both point to same service)"
            else
              echo "Using ClusterIPs - Hot=$HOT_ENDPOINT, Standby=$STANDBY_ENDPOINT"
            fi
          else
            # Fallback to external IPs or node IPs
            HOT_ENDPOINT=$(kubectl get svc backend-service --context=$HOT_CTX -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || kubectl get nodes --context=$HOT_CTX -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || echo "localhost")
            STANDBY_ENDPOINT=$(kubectl get svc backend-service --context=$STANDBY_CTX -n default -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || kubectl get nodes --context=$STANDBY_CTX -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || echo "localhost")
            # Add port if not present
            [[ "$HOT_ENDPOINT" != *":"* ]] && HOT_ENDPOINT="${HOT_ENDPOINT}:80"
            [[ "$STANDBY_ENDPOINT" != *":"* ]] && STANDBY_ENDPOINT="${STANDBY_ENDPOINT}:80"
            echo "Using external IPs - Hot=$HOT_ENDPOINT, Standby=$STANDBY_ENDPOINT"
          fi
          
          echo "Hot cluster endpoint: $HOT_ENDPOINT"
          echo "Standby cluster endpoint: $STANDBY_ENDPOINT"
          
          # Update and deploy HAProxy configmap with endpoints
          sed "s|<HOT_CLUSTER_EXTERNAL_IP>|${HOT_ENDPOINT}|g" k8s/haproxy/configmap.yaml | sed "s|<STANDBY_CLUSTER_EXTERNAL_IP>|${STANDBY_ENDPOINT}|g" | kubectl apply --context=$HAPROXY_CTX -f -
          kubectl apply --context=$HAPROXY_CTX -f k8s/haproxy/deployment.yaml
          kubectl apply --context=$HAPROXY_CTX -f k8s/haproxy/service.yaml

      - name: Deploy network policies
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
          echo "ðŸ”’ Deploying network policies..."
          
          # Deploy network policies for hot cluster
          kubectl apply --context=$HOT_CTX -f k8s/cluster-hot/network-policy.yaml
          echo "âœ… Network policy applied to hot cluster"
          
          # Deploy network policies for standby cluster
          kubectl apply --context=$STANDBY_CTX -f k8s/cluster-standby/network-policy.yaml
          echo "âœ… Network policy applied to standby cluster"
          
          # Deploy network policy for HAProxy
          kubectl apply --context=$HAPROXY_CTX -f k8s/haproxy/network-policy.yaml
          echo "âœ… Network policy applied to HAProxy"
          
          # Verify network policies are applied
          echo ""
          echo "Network policies status:"
          kubectl get networkpolicies --context=$HOT_CTX -n default || echo "No network policies in hot cluster"
          kubectl get networkpolicies --context=$STANDBY_CTX -n default || echo "No network policies in standby cluster"
          kubectl get networkpolicies --context=$HAPROXY_CTX -n default || echo "No network policies in HAProxy cluster"

      - name: Wait for deployments
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
          echo "Waiting for backend deployments..."
          kubectl wait --for=condition=available --timeout=300s deployment/backend-service-hot --context=$HOT_CTX -n default || kubectl wait --for=condition=available --timeout=300s deployment/backend-service --context=$HOT_CTX -n default || true
          kubectl wait --for=condition=available --timeout=300s deployment/backend-service --context=$STANDBY_CTX -n default || true
          
          echo "Waiting for HAProxy deployment..."
          # Wait for HAProxy pod to be running (not just available)
          for i in {1..30}; do
            HAPROXY_READY=$(kubectl get pods --context=$HAPROXY_CTX -l app=haproxy -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "Unknown")
            if [ "$HAPROXY_READY" = "Running" ]; then
              echo "âœ… HAProxy pod is running"
              break
            fi
            echo "Waiting for HAProxy... ($i/30) Current status: $HAPROXY_READY"
            sleep 2
          done
          
          # Check HAProxy pod status
          HAPROXY_POD=$(kubectl get pods --context=$HAPROXY_CTX -l app=haproxy -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$HAPROXY_POD" ]; then
            echo "HAProxy pod: $HAPROXY_POD"
            kubectl get pod $HAPROXY_POD --context=$HAPROXY_CTX -o wide || true
          fi

      - name: Setup persistent HAProxy port-forward
        run: |
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
<<<<<<< HEAD
          # Wait for HAProxy service to be ready
          echo "Waiting for HAProxy service..."
          for i in {1..30}; do
            if kubectl get svc haproxy --context=$HAPROXY_CTX &>/dev/null; then
              echo "âœ… HAProxy service exists"
              break
            fi
            echo "Waiting for HAProxy service... ($i/30)"
            sleep 2
          done
=======
          # Verify HAProxy service exists
          if ! kubectl get svc haproxy --context=$HAPROXY_CTX &>/dev/null; then
            echo "âŒ HAProxy service not found, skipping port-forward setup" >> $GITHUB_STEP_SUMMARY
            echo "HAProxy service may not be deployed yet" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi
>>>>>>> production-ready-fixes
          
          # Create bin directory if it doesn't exist
          mkdir -p ~/bin
          
          # Stop any existing port-forwards
          pkill -f "kubectl port-forward.*haproxy" || true
          pkill -f "keep-haproxy-forward" || true
          sleep 2
          
          # Create persistent port-forward script
          cat > ~/bin/keep-haproxy-forward.sh << EOF
          #!/bin/bash
          while true; do
            kubectl port-forward --context=$HAPROXY_CTX svc/haproxy 9090:9090 2>&1 | tee -a /tmp/haproxy-portforward.log
            echo "Port-forward disconnected, reconnecting in 5 seconds..." >> /tmp/haproxy-portforward.log
            sleep 5
          done
          EOF
          
          chmod +x ~/bin/keep-haproxy-forward.sh
          
          # Start the persistent port-forward in background
          nohup ~/bin/keep-haproxy-forward.sh > /dev/null 2>&1 &
          
<<<<<<< HEAD
          # Wait and verify port-forward is working
          echo "Waiting for port-forward to be ready..."
          for i in {1..20}; do
            if curl -s -f http://localhost:9090/healthz > /dev/null 2>&1; then
              echo "âœ… HAProxy port-forward is working"
              break
            fi
            echo "Waiting for port-forward... ($i/20)"
            sleep 2
          done
          
          if curl -s -f http://localhost:9090/healthz > /dev/null 2>&1; then
            echo "âœ… Persistent HAProxy port-forward started successfully" >> $GITHUB_STEP_SUMMARY
            echo "âœ… HAProxy accessible at http://localhost:9090" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸  Port-forward may not be working - check HAProxy pod status" >> $GITHUB_STEP_SUMMARY
            kubectl get pods --context=$HAPROXY_CTX -l app=haproxy >> $GITHUB_STEP_SUMMARY || true
=======
          # Wait and verify port-forward is actually working
          echo "Waiting for HAProxy port-forward to be ready..."
          PORT_READY=false
          for i in {1..30}; do
            if curl -s -f http://localhost:9090/healthz > /dev/null 2>&1; then
              echo "âœ… HAProxy port-forward is ready!" >> $GITHUB_STEP_SUMMARY
              echo "âœ… HAProxy accessible at http://localhost:9090" >> $GITHUB_STEP_SUMMARY
              PORT_READY=true
              break
            fi
            sleep 2
          done
          
          if [ "$PORT_READY" = "false" ]; then
            echo "âš ï¸  HAProxy port-forward not ready after 60 seconds" >> $GITHUB_STEP_SUMMARY
            echo "Checking port-forward logs:" >> $GITHUB_STEP_SUMMARY
            tail -20 /tmp/haproxy-portforward.log >> $GITHUB_STEP_SUMMARY 2>&1 || echo "No logs available" >> $GITHUB_STEP_SUMMARY
            echo "Checking if HAProxy pod is running:" >> $GITHUB_STEP_SUMMARY
            kubectl get pods --context=$HAPROXY_CTX -l app=haproxy >> $GITHUB_STEP_SUMMARY 2>&1 || true
>>>>>>> production-ready-fixes
          fi

      - name: Verify pods are created
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          
<<<<<<< HEAD
          # Check if test file exists
          if [ ! -f "tests/test_deployment.py" ]; then
            echo "âš ï¸  test_deployment.py not found, skipping basic tests" >> $GITHUB_STEP_SUMMARY
            echo "This is expected if test files were removed for production" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi
          
          # Install requests if not available
          pip3 install --user requests --quiet 2>/dev/null || pip3 install requests --break-system-packages --quiet || true
          
          # Set up port-forwards for direct cluster access
          kubectl port-forward --context=$HOT_CTX svc/backend-service 8080:80 > /dev/null 2>&1 &
          kubectl port-forward --context=$STANDBY_CTX svc/backend-service 8081:80 > /dev/null 2>&1 &
          
          # Verify port-forwards are working
          echo "Waiting for port-forwards to be ready..."
          for i in {1..10}; do
            if curl -s -f http://localhost:8080/healthz > /dev/null 2>&1 && \
               curl -s -f http://localhost:8081/healthz > /dev/null 2>&1; then
              echo "âœ… Port-forwards ready"
              break
            fi
            if [ $i -eq 10 ]; then
              echo "âš ï¸  Port-forwards not ready after 10 attempts, continuing anyway"
            fi
            sleep 1
          done
          
          # Set environment variables
          export HOT_URL="http://localhost:8080"
          export STANDBY_URL="http://localhost:8081"
          export HAPROXY_URL="http://localhost:9090"
          export PYTHONPATH="${HOME}/Library/Python/3.14/lib/python/site-packages:${PYTHONPATH:-}"
          
          # Run basic tests (non-blocking)
          python3 tests/test_deployment.py || echo "âš ï¸  Basic tests had some failures (non-blocking)"

      - name: Run failover tests
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
          # Check if HAProxy port-forward is working
          if ! curl -s -f http://localhost:9090/healthz > /dev/null 2>&1; then
            echo "âŒ HAProxy not accessible at localhost:9090" >> $GITHUB_STEP_SUMMARY
            echo "Skipping failover tests - HAProxy port-forward may not be ready" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi
          
          # Check if test file exists
          if [ ! -f "tests/test_failover.py" ]; then
            echo "âš ï¸  test_failover.py not found, skipping failover tests" >> $GITHUB_STEP_SUMMARY
            echo "This is expected if test files were removed for production" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi
          
          # Install requests if not available
          pip3 install --user requests --quiet 2>/dev/null || pip3 install requests --break-system-packages --quiet || true
          
          # Set environment variables
          export HAPROXY_URL="http://localhost:9090"
          export NAMESPACE_HOT="default"
          export NAMESPACE_STANDBY="default"
          export PYTHONPATH="${HOME}/Library/Python/3.14/lib/python/site-packages:${PYTHONPATH:-}"
          
          echo "ðŸ”„ Running failover tests..." >> $GITHUB_STEP_SUMMARY
          echo "   - Test 1: Scale hot pods to 0, verify failover to standby" >> $GITHUB_STEP_SUMMARY
          echo "   - Test 2: Scale hot pods back up, verify recovery" >> $GITHUB_STEP_SUMMARY
          
          # Run failover tests (non-blocking for now)
          python3 tests/test_failover.py || echo "âš ï¸  Failover tests had some failures (non-blocking)"
=======
          echo "Checking pods in hot cluster..."
          HOT_PODS=$(kubectl get pods --context=$HOT_CTX -n default -l 'app in (backend-service,backend-service-hot)' --no-headers 2>/dev/null | wc -l | tr -d ' ')
          if [ "$HOT_PODS" -gt 0 ]; then
            echo "âœ… Hot cluster: $HOT_PODS pod(s) found"
            kubectl get pods --context=$HOT_CTX -n default -l 'app in (backend-service,backend-service-hot)'
          else
            echo "âŒ Hot cluster: No pods found"
            exit 1
          fi
          
          echo ""
          echo "Checking pods in standby cluster..."
          STANDBY_PODS=$(kubectl get pods --context=$STANDBY_CTX -n default -l app=backend-service --no-headers 2>/dev/null | wc -l | tr -d ' ')
          if [ "$STANDBY_PODS" -gt 0 ]; then
            echo "âœ… Standby cluster: $STANDBY_PODS pod(s) found"
            kubectl get pods --context=$STANDBY_CTX -n default -l app=backend-service
          else
            echo "âŒ Standby cluster: No pods found"
            exit 1
          fi
          
          echo ""
          echo "âœ… Basic verification: Pods are created in both clusters"
>>>>>>> 52a718bd9ccd315d3dd078a1e3e9cff405413192

      - name: Deployment summary
        run: |
          HOT_CTX="${{ steps.detect-contexts.outputs.hot_context }}"
          STANDBY_CTX="${{ steps.detect-contexts.outputs.standby_context }}"
          HAPROXY_CTX="${{ steps.detect-contexts.outputs.haproxy_context }}"
          
          echo "## ðŸš€ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "Using contexts - Hot: $HOT_CTX, Standby: $STANDBY_CTX, HAProxy: $HAPROXY_CTX" >> $GITHUB_STEP_SUMMARY
          kubectl get pods --context=$HOT_CTX -n default -l app=backend-service >> $GITHUB_STEP_SUMMARY || true
          kubectl get pods --context=$STANDBY_CTX -n default -l app=backend-service >> $GITHUB_STEP_SUMMARY || true
          kubectl get pods --context=$HAPROXY_CTX -l app=haproxy >> $GITHUB_STEP_SUMMARY || true
